# SAM-based Segmentation with Domain Adaptation (SMGwithDA)

A comprehensive pipeline for generating segmentation masks from bounding boxes using the Segment Anything Model (SAM) with unsupervised domain adaptation. This solution is designed to be generalized across different datasets while optimizing for cluttered forest environment scenarios.

## ğŸ¯ Project Overview

This project implements a sophisticated segmentation pipeline that:
- Uses Meta's SAM (Segment Anything Model) as the foundation
- Applies unsupervised domain adaptation for cross-domain generalization
- Generates high-quality segmentation masks from bounding box annotations
- Optimizes for cluttered forest environments while maintaining generalization capability
- Supports both CUDA GPU acceleration and CPU fallback

## ğŸ—ï¸ Architecture

The pipeline follows a 9-step process:

1. **Environment Setup** - CUDA verification, dependency management, SAM model loading
2. **Data Ingestion** - Multi-domain dataset loading and preprocessing
3. **Zero-Shot Segmentation** - Initial mask generation using SAM
4. **Feature Extraction** - Deep feature extraction for domain adaptation
5. **Domain Alignment** - Adversarial training for domain confusion
6. **Self-Training** - Iterative pseudo-labeling on target domain
7. **Post-Processing** - CRF and morphological refinement
8. **Validation** - Performance evaluation and early stopping
9. **Inference Pipeline** - Production-ready deployment module

## ğŸ“ Project Structure

```
SMGwithDA/
â”œâ”€â”€ src/                          # Source code modules
â”‚   â”œâ”€â”€ environment_setup.py      # Environment and dependency management
â”‚   â”œâ”€â”€ sam_setup.py              # SAM model setup and utilities
â”‚   â”œâ”€â”€ data_preprocessing.py     # Data loading and preprocessing (Step 2)
â”‚   â”œâ”€â”€ zero_shot_segmentation.py # Initial SAM mask generation (Step 3)
â”‚   â”œâ”€â”€ feature_extraction.py     # Feature extraction for DA (Step 4)
â”‚   â”œâ”€â”€ domain_adaptation.py      # Domain alignment module (Step 5)
â”‚   â”œâ”€â”€ self_training.py          # Iterative self-training (Step 6)
â”‚   â”œâ”€â”€ post_processing.py        # CRF and morphological ops (Step 7)
â”‚   â”œâ”€â”€ validation.py             # Evaluation metrics (Step 8)
â”‚   â””â”€â”€ inference_pipeline.py     # Final inference module (Step 9)
â”œâ”€â”€ dataset/                      # Dataset directory
â”‚   â”œâ”€â”€ source/                   # Source domain data
â”‚   â”‚   â”œâ”€â”€ images/              # Source images
â”‚   â”‚   â”œâ”€â”€ annotations/         # Bounding box annotations
â”‚   â”‚   â””â”€â”€ masks/               # Ground truth masks (optional)
â”‚   â””â”€â”€ target/                   # Target domain data
â”‚       â”œâ”€â”€ images/              # Target images (unlabeled)
â”‚       â””â”€â”€ annotations/         # Target bounding boxes
â”œâ”€â”€ models/                       # Model checkpoints
â”‚   â”œâ”€â”€ sam_vit_b_01ec64.pth     # SAM base model checkpoint
â”‚   â””â”€â”€ adapted_models/          # Domain-adapted model saves
â”œâ”€â”€ outputs/                      # Generated outputs
â”‚   â”œâ”€â”€ masks/                   # Generated segmentation masks
â”‚   â”œâ”€â”€ visualizations/          # Result visualizations
â”‚   â””â”€â”€ metrics/                 # Evaluation results
â”œâ”€â”€ logs/                        # Training and inference logs
â”œâ”€â”€ main_pipeline.ipynb          # Main Jupyter notebook workflow
â”œâ”€â”€ requirements.txt             # Python dependencies
â””â”€â”€ README.md                    # This file
```

## ğŸš€ Quick Start

### 1. Environment Setup

```bash
# Clone the repository (if not already done)
cd SMGwithDA

# Install dependencies
pip install -r requirements.txt

# Optional: Create conda environment
conda create -n smgda python=3.8
conda activate smgda
pip install -r requirements.txt
```

### 2. Launch the Pipeline

```bash
# Start Jupyter notebook
jupyter notebook main_pipeline.ipynb
```

### 3. Run Step-by-Step

The main notebook (`main_pipeline.ipynb`) guides you through each step:
1. Execute the environment setup cells
2. Follow the step-by-step instructions
3. Wait for confirmation before proceeding to next steps

## ğŸ’¾ Dataset Format

### Source Dataset Structure
```
dataset/source/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ image_001.jpg
â”‚   â”œâ”€â”€ image_002.jpg
â”‚   â””â”€â”€ ...
â”œâ”€â”€ annotations/
â”‚   â””â”€â”€ annotations.json         # COCO format or custom JSON
â””â”€â”€ masks/                       # Optional ground truth masks
    â”œâ”€â”€ image_001_mask.png
    â””â”€â”€ ...
```

### Target Dataset Structure
```
dataset/target/
â”œâ”€â”€ images/
â”‚   â”œâ”€â”€ target_001.jpg
â”‚   â””â”€â”€ ...
â””â”€â”€ annotations/
    â””â”€â”€ annotations.json         # Bounding boxes only
```

### Annotation Format
Supports COCO format or custom JSON:
```json
{
  "images": [
    {
      "id": 1,
      "file_name": "image_001.jpg",
      "width": 1024,
      "height": 768
    }
  ],
  "annotations": [
    {
      "id": 1,
      "image_id": 1,
      "bbox": [x, y, width, height],
      "category_id": 1
    }
  ]
}
```

## âš™ï¸ Configuration

### SAM Model Options
- `vit_b`: Base model (~350MB) - Fastest, good for development
- `vit_l`: Large model (~1.2GB) - Better accuracy
- `vit_h`: Huge model (~2.4GB) - Best accuracy

### Hardware Requirements
- **Minimum**: 8GB RAM, CPU-only (slow)
- **Recommended**: 16GB RAM, NVIDIA GPU with 8GB+ VRAM
- **Optimal**: 32GB RAM, NVIDIA RTX 3080/4080+ with 12GB+ VRAM

### CUDA Support
The pipeline automatically detects and uses CUDA if available:
- Supports CUDA 11.x and 12.x
- Falls back to CPU if CUDA unavailable
- Multi-GPU support (experimental)

## ğŸ”§ Advanced Usage

### Custom Dataset Integration
```python
from src.data_preprocessing import DataPreprocessor

# Initialize with custom dataset
preprocessor = DataPreprocessor(
    source_path="path/to/source",
    target_path="path/to/target",
    annotation_format="coco"  # or "custom"
)
```

### Model Configuration
```python
from src.sam_setup import create_sam_setup

# Use different SAM model
sam_setup = create_sam_setup(
    model_type='vit_l',  # vit_b, vit_l, vit_h
    device='cuda:0'      # specific GPU
)
```

### Domain Adaptation Parameters
```python
# In domain_adaptation.py
adaptation_config = {
    'learning_rate': 1e-4,
    'adversarial_weight': 0.1,
    'batch_size': 16,
    'num_epochs': 50
}
```

## ğŸ“Š Evaluation Metrics

The pipeline tracks multiple metrics:
- **IoU (Intersection over Union)**: Standard segmentation metric
- **mIoU (mean IoU)**: Average IoU across all objects/classes
- **Dice Coefficient**: Alternative overlap metric
- **Boundary F1**: Boundary accuracy metric
- **Domain Confusion**: Domain adaptation effectiveness

## ğŸ› Troubleshooting

### Common Issues

1. **CUDA Out of Memory**
   ```bash
   # Reduce batch size or use smaller SAM model
   export CUDA_VISIBLE_DEVICES=0
   ```

2. **SAM Checkpoint Download Fails**
   ```bash
   # Manual download
   wget https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth -P models/
   ```

3. **Import Errors**
   ```bash
   # Reinstall dependencies
   pip install --upgrade segment-anything transformers torch torchvision
   ```

### Performance Optimization

1. **Use Mixed Precision Training**
   ```python
   # Enable in domain adaptation
   use_amp = True
   ```

2. **Optimize Data Loading**
   ```python
   # Increase num_workers
   dataloader = DataLoader(..., num_workers=8)
   ```

## ğŸ¤ Contributing

1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open a Pull Request

## ğŸ“„ License

This project is licensed under the MIT License - see the [LICENSE](LICENSE) file for details.

## ğŸ™ Acknowledgments

- **Meta AI** for the Segment Anything Model
- **PyTorch** team for the deep learning framework
- **Domain adaptation** research community
- **Forest environment** dataset contributors

## ğŸ“ Support

For issues and questions:
1. Check the troubleshooting section
2. Review closed issues on GitHub
3. Open a new issue with detailed description
4. Include system info and error logs

---

**Developed with â¤ï¸ for Computer Vision by Kazi Fahim Tahmid** ğŸš€

